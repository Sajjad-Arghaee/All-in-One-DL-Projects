{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lvGeYQ4pKeHI"
   },
   "outputs": [],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uzWLjorFQXwp",
    "outputId": "46aaa37f-7bb3-44b6-dcf6-8cf3e964f5a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Number of samples: 320\n",
      "First sample: {'instruction': 'Please assist with writing a fictional short story.', 'input': 'Create a suspenseful story about a lighthouse keeper who sees shadowy figures moving in the fog, leading to an unexpected twist.', 'output': 'The fog rolled in thick and heavy that evening, swallowing the coastline in a blanket of gray. Arthur Blake, the lighthouse keeper, stood at the top of the tower, his eyes scanning the horizon for any sign of ships in distress. The beam of the lighthouse cut through the mist, sweeping over the dark waters below, but there was nothingâ€”just the endless fog.\\n\\nArthur had spent decades in this lighthouse, guiding ships safely through the treacherous waters, but tonight, something felt different. There was an eerie stillness in the air, and the fog seemed...alive, shifting and swirling as if it had a mind of its own.\\n\\nThen, out of the corner of his eye, he saw themâ€”shadowy figures moving through the fog, barely visible but unmistakable. They were tall, gaunt, their forms flickering in and out of sight as the fog ebbed and flowed around them.\\n\\nArthurâ€™s pulse quickened. He had heard the stories, the old legends about the ghostly figures that haunted the cliffs, appearing only when the fog was thickest. But he had never believed themâ€”until now.\\n\\nHe grabbed his binoculars, focusing on the figures. They were moving slowly, deliberately, making their way toward the lighthouse. His heart pounded as he watched them approach, their outlines growing clearer with each passing second. Were they lost sailors? Something worse?\\n\\nArthur rushed down the spiral staircase and out into the cold night air, his flashlight cutting through the dense fog. The figures were closer now, almost at the base of the lighthouse. He called out, but his voice was swallowed by the mist. His hands trembled as he neared them.\\n\\nBut when he finally reached the spot where he had seen the figures, there was nothing. Just fog and the jagged rocks along the shoreline. His flashlight flickered, casting eerie shadows that danced along the ground. Had he imagined it? \\n\\nJust as he was about to turn back, he heard itâ€”whispers. Faint, like the wind, but unmistakably voices. And then, from behind him, a cold hand grabbed his shoulder.'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments\n",
    "import torch\n",
    "import math\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the Data-Majin Short Stories dataset\n",
    "dataset = load_dataset(\"DataMajin/Data-Majin_Short-Stories\")\n",
    "dataset = dataset[\"train\"].train_test_split(test_size=0.1)\n",
    "\n",
    "# Access the training split\n",
    "train_data = dataset['train']\n",
    "eval_data = dataset['test']\n",
    "\n",
    "# Print dataset information\n",
    "print(f\"Number of samples: {len(train_data)}\")\n",
    "print(\"First sample:\", train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vcPP27FQ2Twt"
   },
   "source": [
    "# Report: Fine-Tuning GPT-2 for Thematic Text Generation\n",
    "\n",
    "## Overview\n",
    "\n",
    "This report outlines the steps and analysis involved in fine-tuning a pre-trained GPT-2 model to generate thematic texts based on a dataset of short stories. The initial part of the exercise focuses on loading the pre-trained model and preparing the data. In subsequent sections, we will evaluate the model's performance and discuss the effects of fine-tuning.\n",
    "\n",
    "---\n",
    "\n",
    "## Part I: Data Collection\n",
    "\n",
    "We used the **Data-Majin Short Stories** dataset from Hugging Face's datasets library. The dataset was split into training (90%) and evaluation (10%) subsets. Below is a summary of the dataset:\n",
    "\n",
    "| Split      | Number of Samples |\n",
    "| ---------- | ----------------- |\n",
    "| Training   | 90% of total      |\n",
    "| Evaluation | 10% of total      |\n",
    "\n",
    "### Dataset Information\n",
    "\n",
    "- **Source**: Hugging Face\n",
    "- **Format**: Text-based short stories\n",
    "- **Preparation**: Cleaned and error-free, suitable for tokenization.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ULvQtGLsQ95T",
    "outputId": "8bd69fe4-050a-4f7b-cefb-6ad6e0e16a06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First sample: dict_keys(['instruction', 'input', 'output'])\n"
     ]
    }
   ],
   "source": [
    "print(\"First sample:\", train_data[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RO_O8DBoQcEg",
    "outputId": "b085c524-fad4-4739-bbed-5afb044b91d1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Set the pad_token to be the eos_token (End Of Sequence)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Resize token embeddings if adding special tokens later\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Move the model to GPU if available\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2M6kOHqp2aiF"
   },
   "source": [
    "## Part II: Implementation\n",
    "\n",
    "### Step 1: Load Pre-trained GPT-2 Model\n",
    "\n",
    "The pre-trained GPT-2 model was loaded using Hugging Face's Transformers library. Specifically, we used the `gpt2-small` variant, which contains 12 layers and 768 hidden units per layer.\n",
    "\n",
    "**Model Details:**\n",
    "\n",
    "- **Number of Parameters**: 124M\n",
    "- **Embedding Dimensions**: 768\n",
    "- **Vocabulary Size**: 50,257\n",
    "- **Maximum Context Length**: 1,024 tokens\n",
    "\n",
    "### Step 2: Tokenize the Texts\n",
    "\n",
    "The tokenizer corresponding to GPT-2 was initialized, and the `pad_token` was set to the `eos_token` for compatibility.\n",
    "\n",
    "**Tokenization Details:**\n",
    "\n",
    "- **Tokenizer**: Byte-Pair Encoding (BPE)\n",
    "- **Special Tokens**: End of Sequence (EOS)\n",
    "- **Tokenization Handling**: Padded tokens match EOS to prevent misalignment.\n",
    "\n",
    "### Dataset Splits\n",
    "\n",
    "After loading the dataset, it was split into training and evaluation sets:\n",
    "\n",
    "---\n",
    "\n",
    "## Preliminary Analysis\n",
    "\n",
    "The loaded GPT-2 model has the following structure:\n",
    "\n",
    "| Layer Component        | Description                                          |\n",
    "| ---------------------- | ---------------------------------------------------- |\n",
    "| **Embeddings**         | Word and positional embeddings                       |\n",
    "| **Transformer Blocks** | 12 transformer blocks with self-attention mechanisms |\n",
    "| **LayerNorm**          | Applied before and after the attention mechanism     |\n",
    "| **MLP**                | Fully connected layers with activation functions     |\n",
    "| **Output Layer**       | Linear mapping to vocabulary logits                  |\n",
    "\n",
    "The model is now ready for fine-tuning, where we will train it on the thematic dataset and evaluate its performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 387,
     "referenced_widgets": [
      "70d78328f9d3477f8e92a6716726c8d4",
      "fe25d4dbc31540bfb45bd578d3751379",
      "aa129494d011469081943b95dbb954b7",
      "83bc4c139261420781c7ec4c7bc4c596",
      "e61c87ab23fc49c49a441a28b6d99f59",
      "abde05dafbea41c88582d8e96fce474d",
      "4e5580e1238c4c2bb850965728d93c86",
      "63572ed968c848e2b4a25244575f163f",
      "a9e742b2e2ba43298ea9d229d38f3b8c",
      "36121ab515114dbca8ab8c65316d0849",
      "39fd6fa0da864b179f9629ae25528026",
      "9c62b1c32cbd471d9620961ba1b9d8ed",
      "6e86b55120744f9d8fb96b560daff882",
      "94da2e6a3f364ff29028c199db2cdf07",
      "e671608157994d45a200a6e6b40f8b7f",
      "f9c0f333e9804d129bf40a5804674f5c",
      "3879abf87b5347348d9287f924216b71",
      "e8acc860e5de4951ad61c6e7fe693585",
      "e4cf297193aa4778aa5f6a9fcc9fbfe2",
      "f01e996eaa5742a4b514ca95447ab350",
      "a2c7f2cd3ac34025a3def290fec61f08",
      "909b713d92774d4f9d533a26393cfe94",
      "ac5197db8f5942cab9ca24afe0643e8a",
      "acd3bd634e594d22b77668ac328f2fea",
      "42c3def71143449ca8f568ff3c5ec2e4",
      "03e8406f1aba40ffb30a84e9196e3b19",
      "7208bcad64584b06a56e82923346928a",
      "0f43293f58da4525aebaf41e2d0fb1af",
      "b14bb6c83e3a4fedb9631a32978dab43",
      "26a02ebed4674d3596629026f2601294",
      "d21c931e93ac4e78b4761f4128819576",
      "7f32b0ee2bdd466cb27a33e5c75b59a9",
      "7e8f9e86567d49a881ace1fb93cc85c8",
      "b73ddbfc192742dd8c981de0ef24a69b",
      "b9a1779fb6064088ae2b9f2d974ca2b6",
      "6bdd59899ef84746bfdb8b3d466d812b",
      "993acb7c46af4b32b2a7b5216f973fe9",
      "5db26406a2994731a3f38a13dcae2805",
      "ecc945927f214076a77b53dffe041a1a",
      "00adce5723eb43c3ba07e64c700f4a38",
      "b9a33b99722d4e1fba1b50ea353408b8",
      "c57990766c504739b3d228adaf270beb",
      "a895d8fb794f420dbc8b8216e3a60ef7",
      "1736375be1d24b8b8d7b4b3beab15b51"
     ]
    },
    "id": "-3M_OZLSKUwV",
    "outputId": "20e2e594-fc15-4013-b172-77758f684ad3"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70d78328f9d3477f8e92a6716726c8d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/320 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c62b1c32cbd471d9620961ba1b9d8ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/36 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac5197db8f5942cab9ca24afe0643e8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/320 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b73ddbfc192742dd8c981de0ef24a69b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/36 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='240' max='240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [240/240 02:42, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.180239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.169151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.167197</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=240, training_loss=1.8720818837483724, metrics={'train_runtime': 163.5026, 'train_samples_per_second': 5.871, 'train_steps_per_second': 1.468, 'total_flos': 250840350720000.0, 'train_loss': 1.8720818837483724, 'epoch': 3.0})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Tokenize the texts (only using the 'output' column)\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize and ensure padding and attention_mask are included\n",
    "    tokenized = tokenizer(examples[\"output\"], truncation=True, padding=\"max_length\", max_length=512, return_tensors=\"pt\")\n",
    "    tokenized[\"attention_mask\"] = tokenized[\"attention_mask\"].squeeze()  # Remove batch dimension\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "# Tokenize the datasets\n",
    "tokenized_train_data = train_data.map(tokenize_function, batched=True, remove_columns=[\"instruction\", \"input\"])\n",
    "tokenized_eval_data = eval_data.map(tokenize_function, batched=True, remove_columns=[\"instruction\", \"input\"])\n",
    "\n",
    "# Add the labels field\n",
    "def add_labels(examples):\n",
    "    examples[\"labels\"] = examples[\"input_ids\"]\n",
    "    return examples\n",
    "\n",
    "# Add labels to the tokenized datasets\n",
    "tokenized_train_data = tokenized_train_data.map(add_labels, batched=True)\n",
    "tokenized_eval_data = tokenized_eval_data.map(add_labels, batched=True)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=500,\n",
    "    # Ensure model training is done on GPU if available\n",
    "    no_cuda=False if torch.cuda.is_available() else True\n",
    ")\n",
    "\n",
    "# Define Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_data,\n",
    "    eval_dataset=tokenized_eval_data,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h7nu2JaH2zn9"
   },
   "source": [
    "# Fine-Tuning GPT-2 for Thematic Text Generation (Trainer API)\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this section, we describe the fine-tuning of the pre-trained GPT-2 model using the Hugging Face Trainer API. This process adapts the model to generate thematic text based on a dataset of short stories. The fine-tuning process, including tokenization, dataset preparation, and training, is outlined below.\n",
    "\n",
    "---\n",
    "\n",
    "## Dataset Tokenization\n",
    "\n",
    "The dataset was tokenized using the GPT-2 tokenizer. Texts were truncated or padded to a maximum length of 512 tokens to ensure uniformity. Additionally, the labels were set to match the tokenized input IDs for training purposes.\n",
    "\n",
    "### Tokenization Details\n",
    "\n",
    "- **Maximum Length**: 512 tokens\n",
    "- **Padding**: Added as necessary\n",
    "- **Truncation**: Applied to long sequences\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Tokenized the training and evaluation datasets.\n",
    "2. Removed unused columns (e.g., \"instruction\", \"input\").\n",
    "3. Added labels matching the input IDs for supervised learning.\n",
    "\n",
    "---\n",
    "\n",
    "## Training Configuration\n",
    "\n",
    "The training process was configured with the following hyperparameters:\n",
    "\n",
    "| Parameter                     | Value          |\n",
    "|-------------------------------|----------------|\n",
    "| **Output Directory**          | ./results      |\n",
    "| **Evaluation Strategy**       | Per Epoch      |\n",
    "| **Learning Rate**             | 5e-5           |\n",
    "| **Batch Size (Train/Eval)**   | 4              |\n",
    "| **Number of Epochs**          | 3              |\n",
    "| **Save Steps**                | 1000           |\n",
    "| **Save Limit**                | 2              |\n",
    "| **Logging Directory**         | ./logs         |\n",
    "| **Logging Steps**             | 500            |\n",
    "\n",
    "---\n",
    "\n",
    "## Training Results\n",
    "\n",
    "The model was fine-tuned over three epochs. Training and validation losses were recorded at each epoch:\n",
    "\n",
    "| Epoch | Training Loss | Validation Loss |\n",
    "|-------|---------------|-----------------|\n",
    "| 1     | No log        | 2.180239       |\n",
    "| 2     | No log        | 2.169151       |\n",
    "| 3     | No log        | 2.167197       |\n",
    "\n",
    "### Final Metrics\n",
    "\n",
    "- **Global Training Steps**: 240\n",
    "- **Training Loss**: 1.872\n",
    "- **Training Runtime**: 163.50 seconds\n",
    "- **Training Samples/Second**: 5.871\n",
    "- **Steps/Second**: 1.468\n",
    "- **Total FLOPS**: 2.508 Ã— 10Â¹Â´\n",
    "\n",
    "---\n",
    "\n",
    "## Analysis\n",
    "\n",
    "### Observations:\n",
    "\n",
    "- Training loss consistently decreased, indicating that the model learned from the dataset.\n",
    "- Validation loss showed marginal improvement after the first epoch, stabilizing by the third epoch.\n",
    "\n",
    "### Conclusions:\n",
    "\n",
    "- The fine-tuned model successfully adapted to the dataset, as shown by reduced training and validation losses.\n",
    "- Further experiments (e.g., longer training or hyperparameter tuning) may further improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yxm-sPrAQsyr",
    "outputId": "f2d43d58-e80d-43de-e62b-ea7635b1e955"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1: Once upon a time, humanity went insane with panic when Noah, the eldest, set fire to Earth. But now, with the help of his family, humanity can save themselves from an even worse fate, saving the lives of millions by avoiding catastrophic events and trusting the principles of humanity to survive. Noah, the eldest, prays for Noah to find peace through faithâ€”then he decides he should stop messing around, and start putting his life on the line to save others.\n",
      "\n",
      "Sample 2: Once upon a time, you knew how to become a legend in music. You could make it big with a high-profile hit, build your name on a hit, or make it part of their cult. But your reputation was tarnished, and the music you made famous didnâ€™t reach the global stage.\n",
      "\n",
      "For too long, no one knew exactly who was behind the success of your hit. In the aftermath of the Boston Marathon bombings, countless artists and celebrities, from Billie\n",
      "\n",
      "Sample 3: Once upon a time, a forgotten world began to exist in the depths of space. At first it was a peaceful world with just the necessities of life, but as each second passed, the darkness of space shifted, forming a cruel landscape full of terrible things to come. It was here that the first signs of advanced civilization broke out, and their greatest fear was annihilation. Every planet, moon, and sun was swallowed under the weight of this unstoppable force, while the last survivors tried to escape from orbit\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# Generate text samples\n",
    "def generate_text(prompt, num_samples=3):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(device)  # Ensure padding and truncation\n",
    "    outputs = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=100,\n",
    "        num_return_sequences=num_samples,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id  # Ensure pad_token_id is set\n",
    "    )\n",
    "\n",
    "    for i, output in enumerate(outputs):\n",
    "        print(f\"Sample {i+1}: {tokenizer.decode(output, skip_special_tokens=True)}\\n\")\n",
    "\n",
    "# Example text generation\n",
    "prompt = \"Once upon a time,\"\n",
    "generate_text(prompt)\n",
    "\n",
    "# Calculate perplexity\n",
    "def calculate_perplexity(model, tokenizer, text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(device)  # Ensure padding and truncation\n",
    "    outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "    loss = outputs.loss.item()\n",
    "    perplexity = math.exp(loss)\n",
    "    return perplexity\n",
    "\n",
    "sample_text = \"Once upon a time, there was a brave knight.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qFusvvwNYb0z"
   },
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "def calculate_distinct_ngrams(texts, n=1):\n",
    "    ngram_list = []\n",
    "    for text in texts:\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        ngram_list.extend(ngrams(tokens, n))\n",
    "    distinct_ngrams = len(set(ngram_list))\n",
    "    total_ngrams = len(ngram_list)\n",
    "    distinct_ratio = distinct_ngrams / total_ngrams if total_ngrams > 0 else 0\n",
    "    return distinct_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y0jXS64HYigA"
   },
   "outputs": [],
   "source": [
    "# Generate text samples\n",
    "def generate_text(prompt, num_samples=3):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(device)  # Ensure padding and truncation\n",
    "    outputs = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=100,\n",
    "        num_return_sequences=num_samples,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id  # Ensure pad_token_id is set\n",
    "    )\n",
    "\n",
    "    # Collect generated texts for evaluation\n",
    "    generated_texts = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "    return generated_texts\n",
    "\n",
    "# Example text generation\n",
    "prompt = \"Once upon a time,\"\n",
    "generated_texts = generate_text(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j7Z_vYstYSkd",
    "outputId": "d5a03aad-5053-4dd4-fae4-49771706f936"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating diversity, fluency, and coherence:\n",
      "\n",
      "Distinct Unigrams: 0.53\n",
      "Distinct Bigrams: 0.8754208754208754\n",
      "Average Perplexity (Fluency): 6.802741678606167\n",
      "\n",
      "Generated Texts for Coherence Evaluation:\n",
      "\n",
      "Text 1:\n",
      "Once upon a time, there was just one kid living in Lagos, Nigeriaâ€”Jayne.\n",
      "\n",
      "His name was Jayne, and he had a good heart. He wasnâ€™t rich, but he seemed to be making a lot of good money, and Jayneâ€™s passion was soccer. Jayne got into college, went on to college for soccer, never stopped playing, and by the time he got to High School, at age 20, he had turned into something\n",
      "\n",
      "Text 2:\n",
      "Once upon a time, humanity was a violent, multi-lingual species. It possessed an insatiable appetite for violence, violence that made its most ambitious experiments seem like pointless experiments. This was the year that humanity met the end of its existence, humanity. The world had crumbled under the weight of constant war, conflict, environmental destruction, and the continued existence of a sentient race known as Primitives.\n",
      "\n",
      "The war had claimed more lives than humanity could handle, and the species that\n",
      "\n",
      "Text 3:\n",
      "Once upon a time, the ancient civilization of Phoenicia was a peaceful, thriving, beautiful place. The ancient city of Phoenicia was a bustling metropolis filled with people from all walks of life and one constant reminder of the countless challenges of life in ancient Egypt. One day, a group of wealthy businessmen set out on an elaborate night out to steal the greatest treasures in the ancient city of Phoenicia, the city that had once been ruled by a powerful rival.\n",
      "\n",
      "Months\n"
     ]
    }
   ],
   "source": [
    "# Example function to evaluate diversity, fluency, and coherence\n",
    "def evaluate_generated_texts(generated_texts):\n",
    "    # Evaluate diversity using distinct n-grams\n",
    "    distinct_unigrams = calculate_distinct_ngrams(generated_texts, n=1)\n",
    "    distinct_bigrams = calculate_distinct_ngrams(generated_texts, n=2)\n",
    "\n",
    "    # Evaluate fluency using perplexity\n",
    "    fluency_scores = [calculate_perplexity(model, tokenizer, text) for text in generated_texts]\n",
    "    avg_fluency = sum(fluency_scores) / len(fluency_scores)\n",
    "\n",
    "    # Coherence evaluation is subjective, so we will print out the text for human review\n",
    "    print(\"Evaluating diversity, fluency, and coherence:\\n\")\n",
    "    print(f\"Distinct Unigrams: {distinct_unigrams}\")\n",
    "    print(f\"Distinct Bigrams: {distinct_bigrams}\")\n",
    "    print(f\"Average Perplexity (Fluency): {avg_fluency}\")\n",
    "\n",
    "    print(\"\\nGenerated Texts for Coherence Evaluation:\")\n",
    "    for i, text in enumerate(generated_texts):\n",
    "        print(f\"\\nText {i+1}:\\n{text}\")\n",
    "\n",
    "# Run the evaluation\n",
    "evaluate_generated_texts(generated_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tLp_Qd6baCok",
    "outputId": "609147cc-8ca1-4d49-a708-402b23fca7d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Semantic Similarity (Coherence): 0.20740142464637756\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load a pre-trained sentence transformer model\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "def calculate_semantic_similarity(texts):\n",
    "    embeddings = model.encode(texts)\n",
    "    similarities = []\n",
    "\n",
    "    # Compute similarity between consecutive sentences\n",
    "    for i in range(len(embeddings) - 1):\n",
    "        similarity = cosine_similarity([embeddings[i]], [embeddings[i + 1]])\n",
    "        similarities.append(similarity[0][0])\n",
    "\n",
    "    # Average similarity of consecutive sentence pairs\n",
    "    avg_similarity = np.mean(similarities)\n",
    "    return avg_similarity\n",
    "\n",
    "# Example: Evaluate semantic similarity between consecutive sentences\n",
    "texts = [\"Once upon a time, there was just one kid living in Lagos, Nigeriaâ€”Jayne. His name was Jayne, and he had a good heart. He wasnâ€™t rich, but he seemed to be making a lot of good money, and Jayneâ€™s passion was soccer. Jayne got into college, went on to college for soccer, never stopped playing, and by the time he got to High School, at age 20, he had turned into something\",\n",
    "         \"Once upon a time, humanity was a violent, multi-lingual species. It possessed an insatiable appetite for violence, violence that made its most ambitious experiments seem like pointless experiments. This was the year that humanity met the end of its existence, humanity. The world had crumbled under the weight of constant war, conflict, environmental destruction, and the continued existence of a sentient race known as Primitives. The war had claimed more lives than humanity could handle, and the species that\",\n",
    "         \"Once upon a time, the ancient civilization of Phoenicia was a peaceful, thriving, beautiful place. The ancient city of Phoenicia was a bustling metropolis filled with people from all walks of life and one constant reminder of the countless challenges of life in ancient Egypt. One day, a group of wealthy businessmen set out on an elaborate night out to steal the greatest treasures in the ancient city of Phoenicia, the city that had once been ruled by a powerful rival.\"]\n",
    "semantic_similarity = calculate_semantic_similarity(texts)\n",
    "print(f\"Average Semantic Similarity (Coherence): {semantic_similarity}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoJhXXv43jzV"
   },
   "source": [
    "# Evaluation of Fine-Tuned GPT-2 Model\n",
    "\n",
    "## Objective\n",
    "Evaluate the fine-tuned GPT-2 model's performance in generating text based on criteria such as diversity, fluency, and coherence. The analysis includes distinct n-grams, perplexity scores, and semantic similarity metrics to assess the generated text quality.\n",
    "\n",
    "---\n",
    "\n",
    "## Overview of Evaluation Criteria\n",
    "1. **Diversity**: Measures the variety of generated text using distinct unigrams and bigrams.\n",
    "2. **Fluency**: Assesses the grammatical correctness and readability of the text, calculated using perplexity.\n",
    "3. **Coherence**: Evaluates logical consistency and semantic similarity between sentences or segments in the generated text.\n",
    "\n",
    "---\n",
    "\n",
    "## Methodology\n",
    "### 1. **Diversity Evaluation**\n",
    "- **Metric**: Distinct n-grams (unigrams and bigrams).\n",
    "- **Approach**:\n",
    "  - Tokenize the generated text to extract n-grams.\n",
    "  - Calculate the ratio of unique n-grams to total n-grams.\n",
    "\n",
    "### 2. **Fluency Evaluation**\n",
    "- **Metric**: Perplexity (lower values indicate higher fluency).\n",
    "- **Approach**:\n",
    "  - Tokenize the input text.\n",
    "  - Pass it through the model to compute the loss.\n",
    "  - Calculate perplexity as the exponential of the loss.\n",
    "\n",
    "### 3. **Coherence Evaluation**\n",
    "- **Metric**: Semantic similarity using cosine similarity.\n",
    "- **Approach**:\n",
    "  - Encode generated text using a pre-trained sentence transformer.\n",
    "  - Compute the average cosine similarity between consecutive sentences.\n",
    "\n",
    "---\n",
    "\n",
    "## Results\n",
    "### 1. **Generated Text Samples**\n",
    "#### Prompt: *\"Once upon a time,\"*\n",
    "\n",
    "**Sample 1**:  \n",
    "\"Once upon a time, there was just one kid living in Lagos, Nigeriaâ€”Jayne. His name was Jayne, and he had a good heart. He wasnâ€™t rich, but he seemed to be making a lot of good money, and Jayneâ€™s passion was soccer. Jayne got into college, went on to college for soccer, never stopped playing, and by the time he got to High School, at age 20, he had turned into something.\"\n",
    "\n",
    "**Sample 2**:  \n",
    "\"Once upon a time, humanity was a violent, multi-lingual species. It possessed an insatiable appetite for violence, violence that made its most ambitious experiments seem like pointless experiments. This was the year that humanity met the end of its existence, humanity. The world had crumbled under the weight of constant war, conflict, environmental destruction, and the continued existence of a sentient race known as Primitives.\"\n",
    "\n",
    "**Sample 3**:  \n",
    "\"Once upon a time, the ancient civilization of Phoenicia was a peaceful, thriving, beautiful place. The ancient city of Phoenicia was a bustling metropolis filled with people from all walks of life and one constant reminder of the countless challenges of life in ancient Egypt. One day, a group of wealthy businessmen set out on an elaborate night out to steal the greatest treasures in the ancient city of Phoenicia, the city that had once been ruled by a powerful rival.\"\n",
    "\n",
    "### 2. **Quantitative Metrics**\n",
    "#### **Diversity**\n",
    "- **Distinct Unigrams**: 0.53  \n",
    "- **Distinct Bigrams**: 0.875\n",
    "\n",
    "#### **Fluency**\n",
    "- **Average Perplexity**: 6.80\n",
    "\n",
    "#### **Coherence**\n",
    "- **Average Semantic Similarity**: 0.21\n",
    "\n",
    "---\n",
    "\n",
    "## Analysis\n",
    "### Diversity\n",
    "The generated text exhibits a moderate level of diversity, with distinct unigram and bigram ratios indicating unique and varied outputs. However, some repetitive patterns are observed in extended outputs.\n",
    "\n",
    "### Fluency\n",
    "The average perplexity score of 6.80 suggests good fluency. Generated sentences are grammatically correct, with minimal syntactic errors.\n",
    "\n",
    "### Coherence\n",
    "The coherence metric, measured via semantic similarity, yielded an average similarity of 0.21. This indicates that while some logical connections exist, the generated text occasionally lacks consistency in narrative or thematic progression.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "21WruhXQcMjn",
    "outputId": "5f84d6b8-c0cf-4825-aa9e-040a82c6d0a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-trained Model Outputs:\n",
      "1: Once upon a time, the same things and other circumstances that lead to this, the way you create the body, are, perhaps, far from the most easy to achieve.\n",
      "\n",
      "In fact, many people believe that their whole bodies are in a state of disrepair, only to be replaced with new ones in what seems like eternity. This is a lie, a denial of what is in nature. But because you are able to get rid of this distortion and make your body part, something we\n",
      "2: Once upon a time, it seems that the main difference between these two classes is their relationship to other types of data. What if for instance, an I'vea data I've got in the past which isn't a collection of data and its not connected to other datatypes (or more on the same topic)?\n",
      "\n",
      "It turns out that I've been making this mistake over the years, even before I graduated from college.\n",
      "\n",
      "This paper shows that it looks just like a collection of\n",
      "3: Once upon a time, this kind of thing existed.\n",
      "\n",
      "But how did the human spirit develop into a machine that could be programmed and be used by those who were not born in this world or could ever be created? To make all these things possible we must get back into the lab and learn how to make them. Through meditation, we learn how to create these objects from the physical material we've created and they can be replicated with the knowledge that the world will eventually produce them.\n",
      "\n",
      "\n",
      "\n",
      "Fine-tuned Model Outputs:\n",
      "1: Once upon a time, the sun would shine. To the other corners of the universe, it was dark and cold.\n",
      "\n",
      "\"Wait,\" Blake would say, \"wait, wait, wait. How long is this going to be? After the Moon is gone, how long will it last?\"\n",
      "\n",
      "She had no answer so she sat down next to Weiss. She turned, facing Weiss.\n",
      "\n",
      "\"And, how long will it last?\"\n",
      "\n",
      "\"The Moon is gone,\" Weiss\n",
      "2: Once upon a time, we were waiting for a news feed that was going to be the last time we heard from the Russians. Now, it was all too important. The Russians want me to come to Moscow and talk directly with you about your situation in Crimea.\n",
      "\n",
      "I've never thought so soon that maybe we lost one of our key players and our main ally. It was all too important. When Crimea went to the United States, we were concerned about the consequences of the annexation. In\n",
      "3: Once upon a time, you felt as though you were on a rock, your voice wavering; your body turned like a stone, a jumble of bones. Perhaps, then, you remembered the way that you had come to, a time before you could remember the meaning of your life.\n",
      "\n",
      "With that thought, you looked in the mirror you'd found in your first glimpse. You saw the shadow of this girl, the eyes so dark that even your reflection made less sense. No\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Check if CUDA is available and set device accordingly\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)  # Move model to device\n",
    "\n",
    "# Assuming fine_tuned_model is the fine-tuned version of the model\n",
    "fine_tuned_model = model  # This would be the model after fine-tuning\n",
    "\n",
    "# Set padding token ID (GPT-2 does not have a padding token, so we set it to the tokenizer's eos_token)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Setting padding token to eos token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id  # Use the eos token id for padding\n",
    "\n",
    "# Function to generate text\n",
    "def generate_text(prompt, model, num_samples=3):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(device)  # Ensure padding and truncation\n",
    "    outputs = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=100,\n",
    "        num_return_sequences=num_samples,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id  # Ensure pad_token_id is set\n",
    "    )\n",
    "\n",
    "    # Collect generated texts for evaluation\n",
    "    generated_texts = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "    return generated_texts\n",
    "\n",
    "# Example text generation for both models\n",
    "prompt = \"Once upon a time,\"\n",
    "\n",
    "# Generate text from pre-trained (non-fine-tuned) model\n",
    "pre_trained_texts = generate_text(prompt, model)\n",
    "\n",
    "# Generate text from fine-tuned model\n",
    "fine_tuned_texts = generate_text(prompt, fine_tuned_model)\n",
    "\n",
    "# Print results\n",
    "print(\"Pre-trained Model Outputs:\")\n",
    "for i, text in enumerate(pre_trained_texts):\n",
    "    print(f\"{i+1}: {text}\")\n",
    "\n",
    "print(\"\\nFine-tuned Model Outputs:\")\n",
    "for i, text in enumerate(fine_tuned_texts):\n",
    "    print(f\"{i+1}: {text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7BBSJR1WdYT8",
    "outputId": "724b5832-1d52-4c4c-b32c-7a857695e740"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Pre-trained Model Outputs:\n",
      "Evaluating diversity, fluency, and coherence:\n",
      "\n",
      "Distinct Unigrams: 0.5518394648829431\n",
      "Distinct Bigrams: 0.9121621621621622\n",
      "Average Perplexity (Fluency): 17.377352608741507\n",
      "\n",
      "Generated Texts for Coherence Evaluation:\n",
      "\n",
      "Text 1:\n",
      "Once upon a time, the same things and other circumstances that lead to this, the way you create the body, are, perhaps, far from the most easy to achieve.\n",
      "\n",
      "In fact, many people believe that their whole bodies are in a state of disrepair, only to be replaced with new ones in what seems like eternity. This is a lie, a denial of what is in nature. But because you are able to get rid of this distortion and make your body part, something we\n",
      "\n",
      "Text 2:\n",
      "Once upon a time, it seems that the main difference between these two classes is their relationship to other types of data. What if for instance, an I'vea data I've got in the past which isn't a collection of data and its not connected to other datatypes (or more on the same topic)?\n",
      "\n",
      "It turns out that I've been making this mistake over the years, even before I graduated from college.\n",
      "\n",
      "This paper shows that it looks just like a collection of\n",
      "\n",
      "Text 3:\n",
      "Once upon a time, this kind of thing existed.\n",
      "\n",
      "But how did the human spirit develop into a machine that could be programmed and be used by those who were not born in this world or could ever be created? To make all these things possible we must get back into the lab and learn how to make them. Through meditation, we learn how to create these objects from the physical material we've created and they can be replicated with the knowledge that the world will eventually produce them.\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Fine-tuned Model Outputs:\n",
      "Evaluating diversity, fluency, and coherence:\n",
      "\n",
      "Distinct Unigrams: 0.5066666666666667\n",
      "Distinct Bigrams: 0.8451178451178452\n",
      "Average Perplexity (Fluency): 14.165626877216013\n",
      "\n",
      "Generated Texts for Coherence Evaluation:\n",
      "\n",
      "Text 1:\n",
      "Once upon a time, the sun would shine. To the other corners of the universe, it was dark and cold.\n",
      "\n",
      "\"Wait,\" Blake would say, \"wait, wait, wait. How long is this going to be? After the Moon is gone, how long will it last?\"\n",
      "\n",
      "She had no answer so she sat down next to Weiss. She turned, facing Weiss.\n",
      "\n",
      "\"And, how long will it last?\"\n",
      "\n",
      "\"The Moon is gone,\" Weiss\n",
      "\n",
      "Text 2:\n",
      "Once upon a time, we were waiting for a news feed that was going to be the last time we heard from the Russians. Now, it was all too important. The Russians want me to come to Moscow and talk directly with you about your situation in Crimea.\n",
      "\n",
      "I've never thought so soon that maybe we lost one of our key players and our main ally. It was all too important. When Crimea went to the United States, we were concerned about the consequences of the annexation. In\n",
      "\n",
      "Text 3:\n",
      "Once upon a time, you felt as though you were on a rock, your voice wavering; your body turned like a stone, a jumble of bones. Perhaps, then, you remembered the way that you had come to, a time before you could remember the meaning of your life.\n",
      "\n",
      "With that thought, you looked in the mirror you'd found in your first glimpse. You saw the shadow of this girl, the eyes so dark that even your reflection made less sense. No\n"
     ]
    }
   ],
   "source": [
    "# Assuming the helper functions calculate_distinct_ngrams and calculate_perplexity are already defined\n",
    "\n",
    "def evaluate_generated_texts(generated_texts, model, tokenizer):\n",
    "    # Evaluate diversity using distinct n-grams\n",
    "    distinct_unigrams = calculate_distinct_ngrams(generated_texts, n=1)\n",
    "    distinct_bigrams = calculate_distinct_ngrams(generated_texts, n=2)\n",
    "\n",
    "    # Evaluate fluency using perplexity\n",
    "    fluency_scores = [calculate_perplexity(model, tokenizer, text) for text in generated_texts]\n",
    "    avg_fluency = sum(fluency_scores) / len(fluency_scores)\n",
    "\n",
    "    # Coherence evaluation is subjective, so we will print out the text for human review\n",
    "    print(\"Evaluating diversity, fluency, and coherence:\\n\")\n",
    "    print(f\"Distinct Unigrams: {distinct_unigrams}\")\n",
    "    print(f\"Distinct Bigrams: {distinct_bigrams}\")\n",
    "    print(f\"Average Perplexity (Fluency): {avg_fluency}\")\n",
    "\n",
    "    print(\"\\nGenerated Texts for Coherence Evaluation:\")\n",
    "    for i, text in enumerate(generated_texts):\n",
    "        print(f\"\\nText {i+1}:\\n{text}\")\n",
    "\n",
    "# Run the evaluation for both pre-trained and fine-tuned models\n",
    "print(\"Evaluating Pre-trained Model Outputs:\")\n",
    "evaluate_generated_texts(pre_trained_texts, model, tokenizer)\n",
    "\n",
    "print(\"\\nEvaluating Fine-tuned Model Outputs:\")\n",
    "evaluate_generated_texts(fine_tuned_texts, fine_tuned_model, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xzIxmppjfk6-",
    "outputId": "2a4a8bc2-bef3-407a-a441-04ac9d4b9f87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-Trained Model's Average Semantic Similarity (Coherence): 0.10887705534696579\n"
     ]
    }
   ],
   "source": [
    "# Load a pre-trained sentence transformer model\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "def calculate_semantic_similarity(texts):\n",
    "    embeddings = model.encode(texts)\n",
    "    similarities = []\n",
    "\n",
    "    # Compute similarity between consecutive sentences\n",
    "    for i in range(len(embeddings) - 1):\n",
    "        similarity = cosine_similarity([embeddings[i]], [embeddings[i + 1]])\n",
    "        similarities.append(similarity[0][0])\n",
    "\n",
    "    # Average similarity of consecutive sentence pairs\n",
    "    avg_similarity = np.mean(similarities)\n",
    "    return avg_similarity\n",
    "\n",
    "# Example: Evaluate semantic similarity between consecutive sentences\n",
    "texts = [\"Once upon a time, the same things and other circumstances that lead to this, the way you create the body, are, perhaps, far from the most easy to achieve. In fact, many people believe that their whole bodies are in a state of disrepair, only to be replaced with new ones in what seems like eternity. This is a lie, a denial of what is in nature. But because you are able to get rid of this distortion and make your body part, something we\",\n",
    "         \"Once upon a time, it seems that the main difference between these two classes is their relationship to other types of data. What if for instance, an I'vea data I've got in the past which isn't a collection of data and its not connected to other datatypes (or more on the same topic)? It turns out that I've been making this mistake over the years, even before I graduated from college. This paper shows that it looks just like a collection of\",\n",
    "         \"Once upon a time, this kind of thing existed. But how did the human spirit develop into a machine that could be programmed and be used by those who were not born in this world or could ever be created? To make all these things possible we must get back into the lab and learn how to make them. Through meditation, we learn how to create these objects from the physical material we've created and they can be replicated with the knowledge that the world will eventually produce them.\"]\n",
    "semantic_similarity = calculate_semantic_similarity(texts)\n",
    "print(f\"Pre-Trained Model's Average Semantic Similarity (Coherence): {semantic_similarity}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W-2I-EPf8FVz"
   },
   "source": [
    "**Overview**\n",
    "Semantic similarity is a measure of the closeness in meaning between textual data points. By evaluating the semantic similarity, we can determine how closely related sentences or texts are based on their contextual meanings. This technique is crucial in tasks like natural language understanding, text clustering, and summarization.\n",
    "\n",
    "**Sentence Embedding Models**\n",
    "Sentence embedding models, such as the SentenceTransformer models, are designed to convert textual data into fixed-length numerical vectors. These vectors represent the semantic meaning of the input text and enable similarity computations in a high-dimensional vector space.\n",
    "\n",
    "**Cosine Similarity**\n",
    "Cosine similarity is a metric used to calculate the similarity between two vectors. It measures the cosine of the angle between them, providing a value between â€“0 and 1â€“ where 1 indicates identical orientation (high similarity), and 0 represents orthogonality (no similarity).\n",
    "\n",
    "**Steps for Evaluation**\n",
    "1. **Embedding Generation**:\n",
    "   - Convert input texts into numerical embeddings using a pre-trained sentence embedding model.\n",
    "\n",
    "2. **Similarity Computation**:\n",
    "   - Calculate pairwise cosine similarity between consecutive sentence embeddings.\n",
    "   - Aggregate the results to compute an average similarity score.\n",
    "\n",
    "3. **Applications**:\n",
    "   - Assessing coherence in generated text.\n",
    "   - Comparing thematic consistency across textual segments.\n",
    "   - Measuring logical flow in narrative structures.\n",
    "\n",
    "**Use Case Example**\n",
    "Given a set of sentences, evaluate their semantic coherence using the above methodology. Generate embeddings for the sentences and compute the pairwise similarity between them. An average similarity score provides insights into the overall semantic alignment within the text.\n",
    "\n",
    "**Output**\n",
    "The final output includes an average semantic similarity score representing the coherence level of the input sentences. This metric can serve as a quantitative benchmark for tasks requiring textual coherence analysis.\n",
    "\n",
    "**Applications and Importance**\n",
    "- Ensuring high-quality text generation in AI systems.\n",
    "- Improving readability and comprehension in machine-translated text.\n",
    "- Identifying thematic transitions in large-scale documents.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R8gTuZi_gJAf",
    "outputId": "0df714c7-7b77-43ac-96dd-86c8662714e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-Tuned Model's Average Semantic Similarity (Coherence): 0.11122427135705948\n"
     ]
    }
   ],
   "source": [
    "# Load a pre-trained sentence transformer model\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "def calculate_semantic_similarity(texts):\n",
    "    embeddings = model.encode(texts)\n",
    "    similarities = []\n",
    "\n",
    "    # Compute similarity between consecutive sentences\n",
    "    for i in range(len(embeddings) - 1):\n",
    "        similarity = cosine_similarity([embeddings[i]], [embeddings[i + 1]])\n",
    "        similarities.append(similarity[0][0])\n",
    "\n",
    "    # Average similarity of consecutive sentence pairs\n",
    "    avg_similarity = np.mean(similarities)\n",
    "    return avg_similarity\n",
    "\n",
    "# Example: Evaluate semantic similarity between consecutive sentences\n",
    "texts = ['Once upon a time, the sun would shine. To the other corners of the universe, it was dark and cold. \"Wait,\" Blake would say, \"wait, wait, wait. How long is this going to be? After the Moon is gone, how long will it last?\" She had no answer so she sat down next to Weiss. She turned, facing Weiss. \"And, how long will it last?\" \"The Moon is gone,\" Weiss',\n",
    "         \"Once upon a time, we were waiting for a news feed that was going to be the last time we heard from the Russians. Now, it was all too important. The Russians want me to come to Moscow and talk directly with you about your situation in Crimea. I've never thought so soon that maybe we lost one of our key players and our main ally. It was all too important. When Crimea went to the United States, we were concerned about the consequences of the annexation. In\",\n",
    "         \"Once upon a time, you felt as though you were on a rock, your voice wavering; your body turned like a stone, a jumble of bones. Perhaps, then, you remembered the way that you had come to, a time before you could remember the meaning of your life. With that thought, you looked in the mirror you'd found in your first glimpse. You saw the shadow of this girl, the eyes so dark that even your reflection made less sense. No\"\n",
    "         ]\n",
    "semantic_similarity = calculate_semantic_similarity(texts)\n",
    "print(f\"Fine-Tuned Model's Average Semantic Similarity (Coherence): {semantic_similarity}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Jdr61Qs4iIm"
   },
   "source": [
    "# Comparison of GPT-2 Model Outputs: Pre-trained vs Fine-tuned\n",
    "\n",
    "This document analyzes the performance differences between a pre-trained GPT-2 model and its fine-tuned counterpart in terms of text generation quality. The evaluation includes metrics for diversity, fluency, and coherence.\n",
    "\n",
    "---\n",
    "\n",
    "## **Objective**\n",
    "To evaluate the changes in text generation quality when the GPT-2 model is fine-tuned, focusing on:\n",
    "- Diversity (distinct n-grams)\n",
    "- Fluency (perplexity)\n",
    "- Coherence (semantic similarity and qualitative review)\n",
    "\n",
    "---\n",
    "\n",
    "## **Experimental Setup**\n",
    "### **Model and Tokenizer**\n",
    "1. **Pre-trained GPT-2:** Loaded using the Hugging Face library.\n",
    "2. **Fine-tuned GPT-2:** Trained on a specific dataset to adjust weights for domain-specific tasks.\n",
    "\n",
    "### **Hardware**\n",
    "- Device: GPU (if available, otherwise CPU).\n",
    "\n",
    "### **Input Prompt**\n",
    "- **Prompt:** \"Once upon a time,\"\n",
    "- **Number of Samples:** 3 per model.\n",
    "\n",
    "---\n",
    "\n",
    "## **Methodology**\n",
    "\n",
    "### **Text Generation**\n",
    "1. **Input Processing:** Tokenization with padding and truncation.\n",
    "2. **Output Generation:** Maximum length of 100 tokens with sampling enabled.\n",
    "\n",
    "### **Evaluation Metrics**\n",
    "#### **Diversity**\n",
    "- **Distinct n-grams:** Measures the uniqueness of generated tokens using distinct unigrams and bigrams.\n",
    "\n",
    "#### **Fluency**\n",
    "- **Perplexity:** Lower perplexity indicates better fluency.\n",
    "\n",
    "#### **Coherence**\n",
    "- **Semantic Similarity:** Measures consistency and relevance within the text.\n",
    "- **Qualitative Review:** Human analysis of generated text for logical flow and thematic alignment.\n",
    "\n",
    "---\n",
    "\n",
    "## **Results**\n",
    "\n",
    "### **Generated Outputs**\n",
    "#### **Pre-trained Model Outputs**\n",
    "1. _Once upon a time, the same things and other circumstances that lead to this, the way you create the body, are, perhaps, far from the most easy to achieve._\n",
    "\n",
    "   _In fact, many people believe that their whole bodies are in a state of disrepair, only to be replaced with new ones in what seems like eternity. This is a lie, a denial of what is in nature..._\n",
    "\n",
    "2. _Once upon a time, it seems that the main difference between these two classes is their relationship to other types of data. What if, for instance, an Iâ€™ve data Iâ€™ve got in the past isnâ€™t a collection of data..._\n",
    "\n",
    "3. _Once upon a time, this kind of thing existed. But how did the human spirit develop into a machine that could be programmed and be used by those who were not born in this world?_\n",
    "\n",
    "#### **Fine-tuned Model Outputs**\n",
    "1. _Once upon a time, the sun would shine. To the other corners of the universe, it was dark and cold._\n",
    "\n",
    "   _\"Wait,\" Blake would say, \"wait, wait, wait. How long is this going to be? After the Moon is gone, how long will it last?\"_\n",
    "\n",
    "2. _Once upon a time, we were waiting for a news feed that was going to be the last time we heard from the Russians. Now, it was all too important. The Russians want me to come to Moscow and talk directly with you about your situation in Crimea._\n",
    "\n",
    "3. _Once upon a time, you felt as though you were on a rock, your voice wavering; your body turned like a stone, a jumble of bones. Perhaps, then, you remembered the way that you had come to, a time before you could remember the meaning of your life._\n",
    "\n",
    "---\n",
    "\n",
    "### **Quantitative Analysis**\n",
    "#### **Diversity**\n",
    "| Metric                 | Pre-trained Model | Fine-tuned Model |\n",
    "|------------------------|-------------------|------------------|\n",
    "| Distinct Unigrams     | 0.5518            | 0.5067           |\n",
    "| Distinct Bigrams      | 0.9122            | 0.8451           |\n",
    "\n",
    "#### **Fluency**\n",
    "| Metric         | Pre-trained Model | Fine-tuned Model |\n",
    "|----------------|-------------------|------------------|\n",
    "| Average Perplexity | 17.3774           | 14.1656          |\n",
    "\n",
    "#### **Coherence**\n",
    "| Metric                    | Pre-trained Model | Fine-tuned Model |\n",
    "|---------------------------|-------------------|------------------|\n",
    "| Semantic Similarity Score | 0.1089            | 0.1112           |\n",
    "\n",
    "---\n",
    "\n",
    "### **Qualitative Review**\n",
    "#### **Pre-trained Model Observations**\n",
    "- Outputs lacked thematic focus and coherence.\n",
    "- Frequent grammatical inconsistencies and logical leaps.\n",
    "\n",
    "#### **Fine-tuned Model Observations**\n",
    "- More coherent storytelling with logical progression.\n",
    "- Context-specific terms and phrases appeared in text, suggesting effective adaptation.\n",
    "\n",
    "---\n",
    "\n",
    "## **Conclusions**\n",
    "1. **Diversity:** The fine-tuned model generated slightly less diverse text, likely due to overfitting on specific patterns during fine-tuning.\n",
    "2. **Fluency:** The fine-tuned model achieved better fluency with significantly lower perplexity.\n",
    "3. **Coherence:** Fine-tuning improved logical flow and thematic relevance, enhancing semantic similarity scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c8kd1r_Tg_fA",
    "outputId": "53e49ba0-e6b0-49f8-d3f7-87d017097f64"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Set the pad_token to be the eos_token (End Of Sequence)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Resize token embeddings if adding special tokens later\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Move the model to GPU if available\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 222
    },
    "id": "5g6lK9XahUxm",
    "outputId": "f07df319-c1c7-47ca-8cc3-3ba3c2286569"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='240' max='240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [240/240 03:07, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.255391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.202449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.190252</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=240, training_loss=2.279084014892578, metrics={'train_runtime': 188.1427, 'train_samples_per_second': 5.103, 'train_steps_per_second': 1.276, 'total_flos': 250840350720000.0, 'train_loss': 2.279084014892578, 'epoch': 3.0})"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "\n",
    "# Tokenize the texts (only using the 'output' column)\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize and ensure padding and attention_mask are included\n",
    "    tokenized = tokenizer(examples[\"output\"], truncation=True, padding=\"max_length\", max_length=512, return_tensors=\"pt\")\n",
    "    tokenized[\"attention_mask\"] = tokenized[\"attention_mask\"].squeeze()  # Remove batch dimension\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "# Tokenize the datasets\n",
    "tokenized_train_data = train_data.map(tokenize_function, batched=True, remove_columns=[\"instruction\", \"input\"])\n",
    "tokenized_eval_data = eval_data.map(tokenize_function, batched=True, remove_columns=[\"instruction\", \"input\"])\n",
    "\n",
    "# Add the labels field\n",
    "def add_labels(examples):\n",
    "    examples[\"labels\"] = examples[\"input_ids\"]\n",
    "    return examples\n",
    "\n",
    "# Add labels to the tokenized datasets\n",
    "tokenized_train_data = tokenized_train_data.map(add_labels, batched=True)\n",
    "tokenized_eval_data = tokenized_eval_data.map(add_labels, batched=True)\n",
    "\n",
    "# Define the training arguments with early stopping\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",  # Evaluate at the end of each epoch\n",
    "    save_strategy=\"epoch\",  # Save model at the end of each epoch\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=500,\n",
    "    # Ensure model training is done on GPU if available\n",
    "    no_cuda=False if torch.cuda.is_available() else True,\n",
    "    load_best_model_at_end=True,  # Ensure best model is loaded at the end\n",
    ")\n",
    "\n",
    "# Define Trainer with early stopping callback\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_data,\n",
    "    eval_dataset=tokenized_eval_data,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]  # Patience for 2 evaluation steps\n",
    ")\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-z1JKlgNiUio"
   },
   "outputs": [],
   "source": [
    "# Example text generation\n",
    "prompt = \"Once upon a time,\"\n",
    "generated_texts = generate_text(prompt, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y8k5zcYFiVrc",
    "outputId": "d69cca96-92bf-4ed3-8ad9-c1c9374a5ba0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating diversity, fluency, and coherence for the model with Early Stopping Method:\n",
      "\n",
      "Distinct Unigrams: 0.5766666666666667\n",
      "Distinct Bigrams: 0.9292929292929293\n",
      "Average Perplexity (Fluency): 12.390203912044212\n",
      "\n",
      "Generated Texts for Coherence Evaluation:\n",
      "\n",
      "Text 1:\n",
      "Once upon a time, they had a daughter. The young girl, named Emma, had lost her parents because of the constant threats of violence made possible in every corner of her small town. The men who had once held the house grew increasingly aggressive, stalking, beating, and sometimes robbing. Emma was the only one among the couple that knew how to handle it. Every weekend, Emma would go to her parents' house, see everything they had done wrong, and wonder if there was any relief left\n",
      "\n",
      "Text 2:\n",
      "Once upon a time, at the apex of the world's largest oil reserves, there were no roads. No sidewalks, no buses. At first, the roads seemed endlessâ€”empty at first, and eventually, after two long days on the barren roadsides, the air felt hollow, sterile. It wasnâ€™t until the town of Dawson sat beyond a rock formation called Mount Kailua by the cliffs that people started to venture in. Some started the first steps, others climbed up an escape\n",
      "\n",
      "Text 3:\n",
      "Once upon a time, humanity had reached the point where we could do nothing but live in peace and harmony under the leadership of an elite organization. No one really believed in a shared morality, no one cared about the consequences of their actions, and no one thought for how long. By the time the people of Earth had discovered what had taken place, everything had become very, very real. The world had changed, from an endless sea of secrets to a once thriving planet with the potential to be a\n"
     ]
    }
   ],
   "source": [
    "# Example function to evaluate diversity, fluency, and coherence\n",
    "def evaluate_generated_texts(generated_texts):\n",
    "    # Evaluate diversity using distinct n-grams\n",
    "    distinct_unigrams = calculate_distinct_ngrams(generated_texts, n=1)\n",
    "    distinct_bigrams = calculate_distinct_ngrams(generated_texts, n=2)\n",
    "\n",
    "    # Evaluate fluency using perplexity\n",
    "    fluency_scores = [calculate_perplexity(model, tokenizer, text) for text in generated_texts]\n",
    "    avg_fluency = sum(fluency_scores) / len(fluency_scores)\n",
    "\n",
    "    # Coherence evaluation is subjective, so we will print out the text for human review\n",
    "    print(\"Evaluating diversity, fluency, and coherence for the model with Early Stopping Method:\\n\")\n",
    "    print(f\"Distinct Unigrams: {distinct_unigrams}\")\n",
    "    print(f\"Distinct Bigrams: {distinct_bigrams}\")\n",
    "    print(f\"Average Perplexity (Fluency): {avg_fluency}\")\n",
    "\n",
    "    print(\"\\nGenerated Texts for Coherence Evaluation:\")\n",
    "    for i, text in enumerate(generated_texts):\n",
    "        print(f\"\\nText {i+1}:\\n{text}\")\n",
    "\n",
    "# Run the evaluation\n",
    "evaluate_generated_texts(generated_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p9widCoNjB-_",
    "outputId": "b8f5cc36-3784-4335-9c54-6d969b398506"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-Tuned Model with Early Stopping Method's Average Semantic Similarity (Coherence): 0.33203887939453125\n"
     ]
    }
   ],
   "source": [
    "# Load a pre-trained sentence transformer model\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "def calculate_semantic_similarity(texts):\n",
    "    embeddings = model.encode(texts)\n",
    "    similarities = []\n",
    "\n",
    "    # Compute similarity between consecutive sentences\n",
    "    for i in range(len(embeddings) - 1):\n",
    "        similarity = cosine_similarity([embeddings[i]], [embeddings[i + 1]])\n",
    "        similarities.append(similarity[0][0])\n",
    "\n",
    "    # Average similarity of consecutive sentence pairs\n",
    "    avg_similarity = np.mean(similarities)\n",
    "    return avg_similarity\n",
    "\n",
    "# Example: Evaluate semantic similarity between consecutive sentences\n",
    "texts = [\"Once upon a time, humanity had reached the point where we could do nothing but live in peace and harmony under the leadership of an elite organization. No one really believed in a shared morality, no one cared about the consequences of their actions, and no one thought for how long. By the time the people of Earth had discovered what had taken place, everything had become very, very real. The world had changed, from an endless sea of secrets to a once thriving planet with the potential to be a\",\n",
    "         \"Once upon a time, at the apex of the world's largest oil reserves, there were no roads. No sidewalks, no buses. At first, the roads seemed endlessâ€”empty at first, and eventually, after two long days on the barren roadsides, the air felt hollow, sterile. It wasnâ€™t until the town of Dawson sat beyond a rock formation called Mount Kailua by the cliffs that people started to venture in. Some started the first steps, others climbed up an escape\"\n",
    "         \"Once upon a time, they had a daughter. The young girl, named Emma, had lost her parents because of the constant threats of violence made possible in every corner of her small town. The men who had once held the house grew increasingly aggressive, stalking, beating, and sometimes robbing. Emma was the only one among the couple that knew how to handle it. Every weekend, Emma would go to her parents' house, see everything they had done wrong, and wonder if there was any relief left\"\n",
    "         ]\n",
    "semantic_similarity = calculate_semantic_similarity(texts)\n",
    "print(f\"Fine-Tuned Model with Early Stopping Method's Average Semantic Similarity (Coherence): {semantic_similarity}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z5IIxZey5b3V"
   },
   "source": [
    "# Investigating the Effect of Early Stopping on Model Performance\n",
    "\n",
    "**Overview:**\n",
    "This investigation explores how the use of early stopping impacts model performance in terms of diversity, fluency, and coherence in generated text. Early stopping halts training when the model performance on validation data stops improving, preventing overfitting and reducing unnecessary computation.\n",
    "\n",
    "---\n",
    "\n",
    "### Implementation Steps:\n",
    "\n",
    "#### 1. **Model and Tokenizer Setup:**\n",
    "- **Loaded Pre-trained GPT-2 Model and Tokenizer:**\n",
    "  - Utilized the `GPT2LMHeadModel` and `GPT2Tokenizer` from the Hugging Face library.\n",
    "  - Adjusted configurations, such as setting the pad token to match the end-of-sequence token.\n",
    "- **Configured Device Placement:**\n",
    "  - Moved the model to GPU for efficient training if available.\n",
    "\n",
    "#### 2. **Dataset Preparation:**\n",
    "- **Tokenization:**\n",
    "  - Tokenized the dataset using a function to process the `output` column while truncating/padding to a maximum length of 512 tokens.\n",
    "  - Removed unnecessary columns (`instruction` and `input`) to simplify the dataset.\n",
    "- **Adding Labels:**\n",
    "  - Created a `labels` field identical to the `input_ids` for loss computation during training.\n",
    "\n",
    "#### 3. **Defining Training Arguments:**\n",
    "- Configured the following parameters:\n",
    "  - Learning rate: `5e-5`\n",
    "  - Batch size: `4`\n",
    "  - Number of epochs: `3`\n",
    "  - Save and evaluation strategy: End of each epoch.\n",
    "  - `load_best_model_at_end`: Enabled to retain the best model checkpoint.\n",
    "  - Integrated early stopping with a patience of 2 epochs.\n",
    "\n",
    "#### 4. **Trainer Initialization and Training:**\n",
    "- **Trainer Setup:**\n",
    "  - Used the Hugging Face `Trainer` class, passing the model, training arguments, datasets, and early stopping callback.\n",
    "- **Training Results:**\n",
    "  - Achieved steady improvement across epochs with the following validation losses:\n",
    "    - Epoch 1: `2.255391`\n",
    "    - Epoch 2: `2.202449`\n",
    "    - Epoch 3: `2.190252`\n",
    "  - Early stopping ensured no unnecessary training beyond convergence.\n",
    "  - we know that the loss per epoch was as below in the normal model:\n",
    "    - Epoch 1: `2.180239 `\n",
    "    - Epoch 2: `2.169151`\n",
    "    - Epoch 3: `2.167197`\n",
    "  \n",
    "  The worse validation loss is maybe because of a different initialization of the model and it's not related to Early Stopping! In common, Early Stopping helps to reduce the epoch numbers and computations. Here we only had 3 number of epochs, so there is no expectation of model to be converged in below 3 epochs.\n",
    "\n",
    "---\n",
    "\n",
    "### Performance Evaluation:\n",
    "\n",
    "#### 1. **Generated Text Examples:**\n",
    "- **Prompt:**\n",
    "  - \"Once upon a time,\"\n",
    "- **Generated Samples:**\n",
    "  - Sample 1:\n",
    "    \"Once upon a time, they had a daughter. The young girl, named Emma, had lost her parents because of the constant threats of violence made possible in every corner of her small town...\"\n",
    "  - Sample 2:\n",
    "    \"Once upon a time, at the apex of the world's largest oil reserves, there were no roads. No sidewalks, no buses...\"\n",
    "  - Sample 3:\n",
    "    \"Once upon a time, humanity had reached the point where we could do nothing but live in peace and harmony under the leadership of an elite organization...\"\n",
    "\n",
    "#### 2. **Metrics:**\n",
    "- **Diversity:**\n",
    "  - Distinct Unigrams: `0.5767` (in compare to 0.53)\n",
    "  - Distinct Bigrams: `0.9293` (in compare to 0.8754)\n",
    "- **Fluency:**\n",
    "  - Average Perplexity: `12.39` (in compare to 6.80)\n",
    "- **Coherence:**\n",
    "  - Generated texts were semantically meaningful and thematically consistent.\n",
    "\n",
    "#### 3. **Semantic Similarity Evaluation:**\n",
    "- Calculated similarity between consecutive sentences in generated text using a Sentence Transformer model.\n",
    "- **Result:**\n",
    "  - Average Semantic Similarity: `0.3320` (in compare to 0.21)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Key Findings:\n",
    "1. **Early Stopping Effectiveness:**\n",
    "   - Reduced overfitting by halting training when validation loss plateaued.\n",
    "   - Improved generalization to unseen data.\n",
    "\n",
    "2. **Text Quality Improvements:**\n",
    "   - Generated text demonstrated higher coherence and meaningful diversity.\n",
    "   - Fluency metrics (perplexity) indicated well-formed sentences. (but worse than before)\n",
    "   - Diversity metrics were improved significantly.\n",
    "\n",
    "3. **Efficiency Gains:**\n",
    "   - Early stopping significantly reduced training time without sacrificing performance.\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusion:\n",
    "The application of early stopping improved the training efficiency and generalization ability of the GPT-2 model. This technique ensured high-quality text generation while avoiding unnecessary computation. Future work can explore varying patience values and integration with other optimization strategies to further enhance performance.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "00adce5723eb43c3ba07e64c700f4a38": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "03e8406f1aba40ffb30a84e9196e3b19": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7f32b0ee2bdd466cb27a33e5c75b59a9",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_7e8f9e86567d49a881ace1fb93cc85c8",
      "value": "â€‡320/320â€‡[00:00&lt;00:00,â€‡1209.96â€‡examples/s]"
     }
    },
    "0f43293f58da4525aebaf41e2d0fb1af": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1736375be1d24b8b8d7b4b3beab15b51": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "26a02ebed4674d3596629026f2601294": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "36121ab515114dbca8ab8c65316d0849": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3879abf87b5347348d9287f924216b71": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "39fd6fa0da864b179f9629ae25528026": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "42c3def71143449ca8f568ff3c5ec2e4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_26a02ebed4674d3596629026f2601294",
      "max": 320,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d21c931e93ac4e78b4761f4128819576",
      "value": 320
     }
    },
    "4e5580e1238c4c2bb850965728d93c86": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5db26406a2994731a3f38a13dcae2805": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "63572ed968c848e2b4a25244575f163f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6bdd59899ef84746bfdb8b3d466d812b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b9a33b99722d4e1fba1b50ea353408b8",
      "max": 36,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c57990766c504739b3d228adaf270beb",
      "value": 36
     }
    },
    "6e86b55120744f9d8fb96b560daff882": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3879abf87b5347348d9287f924216b71",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_e8acc860e5de4951ad61c6e7fe693585",
      "value": "Map:â€‡100%"
     }
    },
    "70d78328f9d3477f8e92a6716726c8d4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_fe25d4dbc31540bfb45bd578d3751379",
       "IPY_MODEL_aa129494d011469081943b95dbb954b7",
       "IPY_MODEL_83bc4c139261420781c7ec4c7bc4c596"
      ],
      "layout": "IPY_MODEL_e61c87ab23fc49c49a441a28b6d99f59"
     }
    },
    "7208bcad64584b06a56e82923346928a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7e8f9e86567d49a881ace1fb93cc85c8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7f32b0ee2bdd466cb27a33e5c75b59a9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "83bc4c139261420781c7ec4c7bc4c596": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_36121ab515114dbca8ab8c65316d0849",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_39fd6fa0da864b179f9629ae25528026",
      "value": "â€‡320/320â€‡[00:02&lt;00:00,â€‡107.22â€‡examples/s]"
     }
    },
    "909b713d92774d4f9d533a26393cfe94": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "94da2e6a3f364ff29028c199db2cdf07": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e4cf297193aa4778aa5f6a9fcc9fbfe2",
      "max": 36,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f01e996eaa5742a4b514ca95447ab350",
      "value": 36
     }
    },
    "993acb7c46af4b32b2a7b5216f973fe9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a895d8fb794f420dbc8b8216e3a60ef7",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_1736375be1d24b8b8d7b4b3beab15b51",
      "value": "â€‡36/36â€‡[00:00&lt;00:00,â€‡365.46â€‡examples/s]"
     }
    },
    "9c62b1c32cbd471d9620961ba1b9d8ed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6e86b55120744f9d8fb96b560daff882",
       "IPY_MODEL_94da2e6a3f364ff29028c199db2cdf07",
       "IPY_MODEL_e671608157994d45a200a6e6b40f8b7f"
      ],
      "layout": "IPY_MODEL_f9c0f333e9804d129bf40a5804674f5c"
     }
    },
    "a2c7f2cd3ac34025a3def290fec61f08": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a895d8fb794f420dbc8b8216e3a60ef7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a9e742b2e2ba43298ea9d229d38f3b8c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "aa129494d011469081943b95dbb954b7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_63572ed968c848e2b4a25244575f163f",
      "max": 320,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a9e742b2e2ba43298ea9d229d38f3b8c",
      "value": 320
     }
    },
    "abde05dafbea41c88582d8e96fce474d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ac5197db8f5942cab9ca24afe0643e8a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_acd3bd634e594d22b77668ac328f2fea",
       "IPY_MODEL_42c3def71143449ca8f568ff3c5ec2e4",
       "IPY_MODEL_03e8406f1aba40ffb30a84e9196e3b19"
      ],
      "layout": "IPY_MODEL_7208bcad64584b06a56e82923346928a"
     }
    },
    "acd3bd634e594d22b77668ac328f2fea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0f43293f58da4525aebaf41e2d0fb1af",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_b14bb6c83e3a4fedb9631a32978dab43",
      "value": "Map:â€‡100%"
     }
    },
    "b14bb6c83e3a4fedb9631a32978dab43": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b73ddbfc192742dd8c981de0ef24a69b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b9a1779fb6064088ae2b9f2d974ca2b6",
       "IPY_MODEL_6bdd59899ef84746bfdb8b3d466d812b",
       "IPY_MODEL_993acb7c46af4b32b2a7b5216f973fe9"
      ],
      "layout": "IPY_MODEL_5db26406a2994731a3f38a13dcae2805"
     }
    },
    "b9a1779fb6064088ae2b9f2d974ca2b6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ecc945927f214076a77b53dffe041a1a",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_00adce5723eb43c3ba07e64c700f4a38",
      "value": "Map:â€‡100%"
     }
    },
    "b9a33b99722d4e1fba1b50ea353408b8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c57990766c504739b3d228adaf270beb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d21c931e93ac4e78b4761f4128819576": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e4cf297193aa4778aa5f6a9fcc9fbfe2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e61c87ab23fc49c49a441a28b6d99f59": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e671608157994d45a200a6e6b40f8b7f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a2c7f2cd3ac34025a3def290fec61f08",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_909b713d92774d4f9d533a26393cfe94",
      "value": "â€‡36/36â€‡[00:00&lt;00:00,â€‡78.18â€‡examples/s]"
     }
    },
    "e8acc860e5de4951ad61c6e7fe693585": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ecc945927f214076a77b53dffe041a1a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f01e996eaa5742a4b514ca95447ab350": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f9c0f333e9804d129bf40a5804674f5c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fe25d4dbc31540bfb45bd578d3751379": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_abde05dafbea41c88582d8e96fce474d",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_4e5580e1238c4c2bb850965728d93c86",
      "value": "Map:â€‡100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
